<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Saravanan's Blog - Distributed Systems</title><link href="/" rel="alternate"></link><link href="/feeds/distributed-systems.atom.xml" rel="self"></link><id>/</id><updated>2021-08-28T00:00:00+05:30</updated><entry><title>Raft Consensus Algorithm</title><link href="/raft-consensus-algorithm.html" rel="alternate"></link><published>2021-08-28T00:00:00+05:30</published><updated>2021-08-28T00:00:00+05:30</updated><author><name>Saravanan N</name></author><id>tag:None,2021-08-28:/raft-consensus-algorithm.html</id><summary type="html">&lt;h2&gt;Consensus in Distributed Systems&lt;/h2&gt;
&lt;p&gt;In distributed systems, we will have multiple components running on multiple nodes which can communicate over a network, work together as a single system.
For this system to be stable and reliable, these components needs to talk to each other and needs to agree on certain â€¦&lt;/p&gt;</summary><content type="html">&lt;h2&gt;Consensus in Distributed Systems&lt;/h2&gt;
&lt;p&gt;In distributed systems, we will have multiple components running on multiple nodes which can communicate over a network, work together as a single system.
For this system to be stable and reliable, these components needs to talk to each other and needs to agree on certain values. For example, in a distributed database,
when we write data to it, before we save it, we need different nodes on that database to agree on the new data, so that when a read request goes to any of the available nodes, it won't return different data for the same request.&lt;/p&gt;
&lt;p&gt;According to &lt;a href="https://en.wikipedia.org/wiki/Consensus_(computer_science)"&gt;wikipedia&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;A fundamental problem in distributed computing and multi-agent systems is to achieve overall system reliability in the presence of a number of faulty processes. This often requires coordinating processes to reach consensus, or agree on some data value that is needed during computation.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;Raft&lt;/h2&gt;
&lt;p&gt;Raft is a consensus algorithm, that tries solve to this problem without many complications. Raft protocol depends on existence of a leader in the system to operate. Each node can be in one of the following state.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Leader&lt;/li&gt;
&lt;li&gt;Candidate&lt;/li&gt;
&lt;li&gt;Follower&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Initially when the system starts, every node will be in &lt;em&gt;Follower&lt;/em&gt; state. For the system to work properly as expected, there needs to be a leader, when the absence of leader is detected, a new election is held and new leader is elected. After this, system can start accepting requests from client. All the nodes will maintain a append only replication log to which it will record the incoming requests for changes to data in the system, all these changes will be sent to leader and leader will take care of propagating these changes to follower nodes.&lt;/p&gt;
&lt;h2&gt;Leader Election&lt;/h2&gt;
&lt;p&gt;Each election has an unique term number. After detecting the absence of the leader in the system, any node can stand for the next term and can request votes from other available nodes. Each node will wait for a random election timeout before transitioning to &lt;em&gt;Candidate&lt;/em&gt; state from &lt;em&gt;Follwer&lt;/em&gt; state. After the timeout, the node will enter &lt;em&gt;Candidate&lt;/em&gt; state, then it will send vote request to the other nodes in the system. The vote request will have new election term number(current term number will be increased by 1) and last commit index from its log. Before sending the vote requests, the node will vote for itself in the current term. A node can vote only once for a term, when one node receives the vote request, it will vote only for that term only if it had not previously voted for the same term and the commit index mentioned in the vote request is higher or equal to its own commit index. The node which gets majority of votes will become the leader, if there is no majority, that election will be dropped and new election will be held. After the node becomes leader it will start sending hearbeats to other nodes in the system indicating that it has won the election and it is the leader of the cluster for that election term. The leader node will transition to &lt;em&gt;Leader&lt;/em&gt; state and other nodes in the system will transition to &lt;em&gt;Follower&lt;/em&gt; state. The same procedure will be followed incase of failure of leader node.&lt;/p&gt;
&lt;h2&gt;Log Replication&lt;/h2&gt;
&lt;p&gt;All the changes to the system has to go through the leader, whether it is for updating existing data or creating new data or deleting existing data. The leader will receive the request and records the changes in its log in uncommitted state against a new index and then sends &lt;em&gt;AppendEntry&lt;/em&gt; request to its followers with the changes and the new index. The followers will receive the request and makes the changes to its own replication log and sends acknowlegment. When the leader nodes receive the acknowledgement from majority of the nodes, the change will be marked as committed and a response will be sent to the client. After that the leader will send committed index along with the next &lt;em&gt;AppendEntry&lt;/em&gt; message. The follower after receiving this message, will commit the changes and applies the changes to the data storage. When a new node joins the cluster or a follower becomes available after becoming unavailable, the replication log of that node will not be up-to-date. It is the responsibility of the leader node to replay all the missing changes and replicate them in those nodes. If any of the node cannot agree with any of the change in the replication log with what is present in the leader node due to some network issues or any other kind of issues, leader node will force that node to delete those changes and replicate the changes from the master node.&lt;/p&gt;
&lt;h2&gt;Building simple key value store based on raft&lt;/h2&gt;
&lt;p&gt;I tried implementing a simple distributed key value store, which uses the above raft protocol to address the consensus problem in this system.  The functionality of the key value store is simple, given a key (string) and value (string), it will write it to a single json file. I was not considering LSM and MemTable for this. Given a write request, the system needs to accept the values and write it to a json file on every single node in the system and these json files needs to be in sync.&lt;/p&gt;
&lt;p&gt;For communication between nodes, I used gRPC. For accepting requests from a client, a HTTP API was exposed. The client can send a write request to the leader node and can send read requests any of the available nodes. I have written this system in python using async await, gRPC, asyncio library.&lt;/p&gt;
&lt;p&gt;In this system, i decided to reuse AppendEntry request for heartbeat. when there is a write request to the leader node, it will send AppendEntry with message otherwise it will send empty AppendEntry mesages to the follower nodes (which can be considered as heartbeat). &lt;/p&gt;
&lt;p&gt;Code is available at &lt;a href="https://github.com/saravanan-nj/raft-experiment.git"&gt;Github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;We can run a node by running &lt;code&gt;run.py&lt;/code&gt; file. In that file there is config dictionary, which can be used to mention the gRPC port, http port, data directories and other nodes mpresent in the system. &lt;/p&gt;</content><category term="Distributed Systems"></category></entry></feed>